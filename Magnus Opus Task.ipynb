{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAGNUM OPUS ASK\n",
    "#Take two (big) documents. Stem. Vectorise. Compute Similarity\n",
    "doc1 = [\"I ate dinner.\",\"We had a three-course meal.\",\"Brad came to dinner with us.\",\"He loves fish tacos.\",\n",
    "\"In the end, we all felt like we ate too much.\",\"We all agreed; it was a magnificent evening.\",\"I hope that, when I've built up my savings, I'll be able to travel to Mexico.\",\n",
    "\"Did you know that, along with architecture\"]\n",
    "doc2 = [\"Of all the places to travel, Mexico is at the top of my list.\",\"Would you like to travel with me?\",\n",
    " \"Isn't language learning fun?\",\"There is so much to understand.\",\"I love learning!\",\"Sentences come in many shapes and sizes.\"\n",
    "\"Nothing beats a complete sentence.\",\"Once you know all the elements, it's not difficult to pull together a sentence.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "stem1 = []\n",
    "stem2 = []\n",
    "for sentence in doc1:\n",
    "    stem1.append(\" \".join([st.stem(i) for i in sentence.split()]))\n",
    "for sentence in doc2:\n",
    "    stem2.append(\" \".join([st.stem(i) for i in sentence.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector1 = vectorizer.fit_transform(stem1)\n",
    "vector2 = vectorizer.fit_transform(stem2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.21320072\n",
      "  0.19611614]\n",
      " [0.31622777 0.16903085 0.         0.54772256 0.         0.\n",
      "  0.        ]\n",
      " [0.28867513 0.15430335 0.         0.         0.         0.12309149\n",
      "  0.33968311]\n",
      " [0.11785113 0.         0.         0.         0.35355339 0.15075567\n",
      "  0.2773501 ]\n",
      " [0.13074409 0.         0.13867505 0.45291081 0.         0.2508726\n",
      "  0.07692308]\n",
      " [0.17817416 0.28571429 0.18898224 0.15430335 0.         0.22792115\n",
      "  0.        ]\n",
      " [0.41247896 0.18898224 0.125      0.10206207 0.1767767  0.15075567\n",
      "  0.5547002 ]\n",
      " [0.08908708 0.28571429 0.18898224 0.         0.         0.34188173\n",
      "  0.10482848]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vector1.toarray(),vector2.toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Stemming\n",
    "d1 = [\"I ate dinner.\",\"We had a three-course meal.\",\"Brad came to dinner.\",\"He loves fish tacos.\",\n",
    "\"In the end, we all felt like we ate too much.\",\"We all agreed; it was a magnificent.\",\"I hope that, when I've built up my savings, I'll be able to travel to Mexico.\",\n",
    "\"Did you know that, along with architecture\"]\n",
    "d2 = [\"Of all the places to travel, Mexico is at the top of my list.\",\"Would you like to travel with me?\",\n",
    " \"Isn't language learning fun?\",\"There is so much to understand.\",\"I love learning!\",\"Sentences come in many shapes and sizes.\"\n",
    "\"Nothing beats a complete sentence.\",\"Once you know all the elements, it's not difficult to pull together a sentence.\"]\n",
    "vec1 = vectorizer.fit_transform(d1)\n",
    "vec2 = vectorizer.fit_transform(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.21320072\n",
      "  0.19611614]\n",
      " [0.31622777 0.16903085 0.         0.54772256 0.         0.\n",
      "  0.        ]\n",
      " [0.23570226 0.         0.         0.         0.         0.15075567\n",
      "  0.41602515]\n",
      " [0.11785113 0.         0.25       0.         0.35355339 0.15075567\n",
      "  0.13867505]\n",
      " [0.13074409 0.         0.13867505 0.45291081 0.         0.2508726\n",
      "  0.07692308]\n",
      " [0.19245009 0.3086067  0.         0.16666667 0.         0.24618298\n",
      "  0.        ]\n",
      " [0.45732956 0.1833397  0.12126781 0.09901475 0.17149859 0.14625448\n",
      "  0.60540551]\n",
      " [0.08908708 0.28571429 0.18898224 0.         0.         0.34188173\n",
      "  0.10482848]]\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(vec1.toarray(),vec2.toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
